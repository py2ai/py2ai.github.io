

  <!DOCTYPE html>
<html lang="en" itemscope itemtype="https://schema.org/Article">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Canonical URL - redirect www to non-www -->
  <link rel="canonical" href="https://pyshine.com/Markov-Decision-Processes-Explained/">

  <!-- Pinterest & Google verification -->
  <meta name="p:domain_verify" content="fb68a8459fe32b8b072bcd5cc620d125"/>
  <meta name="google-site-verification" content="WAkFPi5cvufClnQetgIl0STmvvwHhVf8jfyENFWhsxU" />
  <meta name="google-site-verification" content="g2pFZlv-92XQD67BPgiVHsvZ4TZH13Ucvmmvvv36kU4" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL | PyShine</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL" />
<meta name="author" content="PyShine Team" />
<meta property="og:locale" content="en" />
<meta name="description" content="Deep dive into Markov Decision Processes (MDPs) - the mathematical foundation of Reinforcement Learning. Learn states, actions, transitions, rewards, and Bellman equations." />
<meta property="og:description" content="Deep dive into Markov Decision Processes (MDPs) - the mathematical foundation of Reinforcement Learning. Learn states, actions, transitions, rewards, and Bellman equations." />
<link rel="canonical" href="http://localhost:4000/Markov-Decision-Processes-Explained/" />
<meta property="og:url" content="http://localhost:4000/Markov-Decision-Processes-Explained/" />
<meta property="og:site_name" content="PyShine" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-02T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"PyShine Team"},"dateModified":"2026-02-02T00:00:00+08:00","datePublished":"2026-02-02T00:00:00+08:00","description":"Deep dive into Markov Decision Processes (MDPs) - the mathematical foundation of Reinforcement Learning. Learn states, actions, transitions, rewards, and Bellman equations.","headline":"Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Markov-Decision-Processes-Explained/"},"name":"{{ page.title escape }}","url":"http://localhost:4000/Markov-Decision-Processes-Explained/"}</script>
<!-- End Jekyll SEO tag -->
  <!-- This will generate the canonical link automatically -->

  <script src="https://analytics.ahrefs.com/analytics.js" data-key="Im0K82YsmEDy08+6wyUIZA" async></script>

  <!-- Favicon & App Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/img/icons/apple-touch-icon.png?v=qA3OXqyw77">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png?v=qA3OXqyw77">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png?v=qA3OXqyw77">
  <link rel="manifest" href="/assets/img/icons/manifest.json?v=qA3OXqyw77">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->

  <!-- App & Theme Info -->
  <meta name="apple-mobile-web-app-title" content="Sleek">
  <meta name="application-name" content="Sleek">
  <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77">
  <meta name="theme-color" content="#ffffff">

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-TCVFBKF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-TCVFBKF');
</script>

  

  <!-- Critical CSS Inline -->
  <style class="inlineCSS">
    h1{color:#313237;margin-top:0;margin-bottom:.5rem}.dark-bg{background-color:#19d319}@media (min-width:48em){.post-card{width:48.4375%;margin-right:3.125%}.post-card:last-of-type,.post-card:nth-child(2n+2){margin-right:0}}html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figure,main{display:block}figure{margin:1em 40px}a{background-color:transparent;-webkit-text-decoration-skip:objects}img{border-style:none}svg:not(:root){overflow:hidden}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{-webkit-box-sizing:border-box;box-sizing:border-box}body{-webkit-overflow-scrolling:touch}*,::after,::before{-webkit-box-sizing:inherit;box-sizing:inherit}.site{display:-webkit-box;display:-ms-flexbox;display:flex;min-height:100vh;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.site__content{-webkit-box-flex:1;-ms-flex:1;flex:1}img{max-width:100%;height:auto;width:auto;vertical-align:middle}figure{margin:0}body{background-color:#fff;font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Hiragino Sans GB","Microsoft YaHei","WenQuanYi Micro Hei",sans-serif;font-size:1rem;line-height:1.5;color:#343851;-webkit-font-smoothing:antialiased;-webkit-text-size-adjust:100%}p{margin-top:0;margin-bottom:1.25rem}h1,h2{color:#313237;margin-top:0;margin-bottom:.5rem}a{color:#277cea;text-decoration:none;border-bottom:1px dashed #277cea}.blur{background:#fff;filter:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg"><filter id="filter"><feGaussianBlur stdDeviation="16" /></filter></svg>#filter');-webkit-filter:blur(1rem);filter:blur(1rem)}.container{padding:0 20px}@media (min-width:0){.container{max-width:auto;margin:0 auto}}@media (min-width:36em){.container{max-width:540px;margin:0 auto}}@media (min-width:48em){.container{max-width:720px;margin:0 auto}}@media (min-width:62em){.container{max-width:960px;margin:0 auto}}@media (min-width:75em){.container{max-width:1170px;margin:0 auto}}.header{background-color:#fff;color:#343851;position:absolute;z-index:4;width:100%;top:0;left:0;will-change:transform;-webkit-transform:translateY(0);transform:translateY(0)}.header a{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-bottom:0}.header__logo{display:-webkit-box;display:-ms-flexbox;display:flex;height:100%;overflow:hidden;padding:19px 0;margin-right:1.25rem;outline:0;border-bottom:0;color:#313237}.header__logo .header__logo--container{width:58px}.header__logo .header__logo--container .logo{fill:currentColor}.header__inner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:3.75em;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.header__links{padding-bottom:.5rem;display:none;position:absolute;top:3.75em;left:0;width:100%;height:auto;background:#fff}.header__link{color:#343851;padding:.938rem 0;border-top:1px solid #ededed}.header__toggle{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:44px;height:100%;background-color:transparent;padding-left:1.25rem}.header__toggle span{display:block;position:relative;margin-top:4px;background-color:#343851;width:100%;height:2px;border-radius:1px}.header__toggle span:first-child{margin-top:0}@media (min-width:62em){.header__toggle{display:none;visibility:hidden}.header__links{position:static;padding:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;visibility:visible;width:auto;height:100%}.header__links-wrapper{display:-webkit-box;display:-ms-flexbox;display:flex;height:100%;padding:0}.header__link{position:relative;padding:.938rem 1rem;border:0;height:100%}.header__link::after{content:"";display:block;position:absolute;left:0;bottom:0;height:3px;width:100%;-webkit-transform:scaleX(0);transform:scaleX(0);background:#277cea}}.post-card{display:block;position:relative;width:100%;min-height:250px;border-radius:4px;overflow:hidden;background-color:#fff;-webkit-box-shadow:0 1px 3px rgba(0,0,0,.08);box-shadow:0 1px 3px rgba(0,0,0,.08);margin-bottom:2.25rem;border-bottom:0}@media (min-width:48em){.post-card{width:48.4375%;margin-right:3.125%}.post-card:nth-child(2n+2){margin-right:0}}@media (min-width:75em){.post-card{width:31.25%;margin-right:3.125%}.post-card:nth-child(2n+2){margin-right:3.125%}}.post-card__label{position:absolute;top:1.5rem;left:1.5rem;z-index:2}.post-card__thumb{margin:0;background:#fff;position:relative;overflow:hidden}.post-card__thumb::after{content:"";display:block;height:0;width:100%;padding-bottom:56.25%}.post-card__thumb>*{position:absolute;top:0;left:0;width:100%;height:100%;display:block}.post-card__inner{padding:1.875rem 1.25rem .625rem;color:#838c8d}.post-card__header{margin-bottom:.75rem}.post-card__header .post-card__meta{font-size:.875rem}.label{padding:0 10px;margin-bottom:1rem;display:inline-block;line-height:20px;font-size:.75rem;text-transform:uppercase;letter-spacing:1px;color:rgba(255,255,255,.8);border:2px solid rgba(255,255,255,.5);border-radius:100px}.hero{margin:3.75rem auto 0;min-height:16.25rem;width:100%;position:relative;background-color:#dde5ea;background-repeat:no-repeat;background-position:50%;background-size:cover}@media (min-width:62em){.hero{margin:0 auto;height:36em}}.hero::before{position:absolute;display:block;content:"";top:0;left:0;width:100%;height:100%;background:rgba(52,56,81,.8)}.hero__wrap{position:absolute;margin:auto;top:50%;left:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);text-align:center;color:rgba(255,255,255,.8);width:100%;max-width:90%;z-index:1}.hero__wrap .hero__title{font-size:1.8em;color:#fff}.blog{background-color:#f9f9f9}.post-list{padding-top:2.5em;display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-flex:1;-ms-flex:1 0 auto;flex:1 0 auto}@media (min-width:48em){.hero__wrap{max-width:40em}.hero__wrap .hero__title{padding:1rem 0;font-size:2.625em;line-height:3.125rem}.post-list{padding-top:5em}}
  </style>

  <!-- Main CSS -->
  <link rel="preload" href="/assets/css/main.css" as="style" onload="this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="/assets/css/main.css"></noscript>

  <script type="text/javascript">
    /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
(function(w){"use strict";if(!w.loadCSS){w.loadCSS=function(){}}
var rp=loadCSS.relpreload={};rp.support=(function(){var ret;try{ret=w.document.createElement("link").relList.supports("preload")}catch(e){ret=!1}
return function(){return ret}})();rp.bindMediaToggle=function(link){var finalMedia=link.media||"all";function enableStylesheet(){link.media=finalMedia}
if(link.addEventListener){link.addEventListener("load",enableStylesheet)}else if(link.attachEvent){link.attachEvent("onload",enableStylesheet)}
setTimeout(function(){link.rel="stylesheet";link.media="only x"});setTimeout(enableStylesheet,3000)};rp.poly=function(){if(rp.support()){return}
var links=w.document.getElementsByTagName("link");for(var i=0;i<links.length;i++){var link=links[i];if(link.rel==="preload"&&link.getAttribute("as")==="style"&&!link.getAttribute("data-loadcss")){link.setAttribute("data-loadcss",!0);rp.bindMediaToggle(link)}}};if(!rp.support()){rp.poly();var run=w.setInterval(rp.poly,500);if(w.addEventListener){w.addEventListener("load",function(){rp.poly();w.clearInterval(run)})}else if(w.attachEvent){w.attachEvent("onload",function(){rp.poly();w.clearInterval(run)})}}
if(typeof exports!=="undefined"){exports.loadCSS=loadCSS}
else{w.loadCSS=loadCSS}}(typeof global!=="undefined"?global:this))

  </script>

  <!-- MathJax -->
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>


  <body class="site" itemscope itemtype="https://schema.org/WebPage">

    
      <!-- Google Tag Manager (noscript) -->
      <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TCVFBKF"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
      </noscript>
      <!-- End Google Tag Manager -->
    

    <header class="header"
  itemscope
  itemtype="http://schema.org/SiteNavigationElement"
  aria-label="Main navigation">

  <div class="container">
    <div class="header__inner">

      <!-- Logo -->
      <a class="header__logo" href="/">
        <div class="header__logo--container">
          <?xml version="1.0" encoding="utf-8"?>
<!-- Generator: Adobe Illustrator 16.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 0)  -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
	 width="24px" height="24px" viewBox="0 0 24 24" enable-background="new 0 0 24 24" xml:space="preserve">
<g>
	<path d="M18.121,9.88l-7.832-7.836c-0.155-0.158-0.428-0.155-0.584,0L1.842,9.913c-0.262,0.263-0.073,0.705,0.292,0.705h2.069v7.042c0,0.227,0.187,0.414,0.414,0.414h3.725c0.228,0,0.414-0.188,0.414-0.414v-3.313h2.483v3.313c0,0.227,0.187,0.414,0.413,0.414h3.726c0.229,0,0.414-0.188,0.414-0.414v-7.042h2.068h0.004C18.331,10.617,18.389,10.146,18.121,9.88 M14.963,17.245h-2.896v-3.313c0-0.229-0.186-0.415-0.414-0.415H8.342c-0.228,0-0.414,0.187-0.414,0.415v3.313H5.032v-6.628h9.931V17.245z M3.133,9.79l6.864-6.868l6.867,6.868H3.133z"></path>

	
	
</g>
</svg>

        </div>
      </a>

      <!-- Navigation -->
      <nav class="header__links">
        <div class="header__links-wrapper">

          
            
              <a class="header__link" href="/">
                Home
              </a>
            
          
            
              <a class="header__link" href="/categories">
                Posts
              </a>
            
          
            

            <!-- Applications Dropdown -->
            <div class="header__dropdown">
              <span class="header__link header__dropdown-toggle">
                Applications
                <span class="dropdown-arrow">â–¾</span>
              </span>

              <div class="header__dropdown-menu">
                
                  <a class="header__dropdown-item"
                     href="/photo_correction">
                    Online Photo Correction
                  </a>
                
                  <a class="header__dropdown-item"
                     href="/photo">
                    Online Passport Photo 20KB
                  </a>
                
                  <a class="header__dropdown-item"
                     href="/finger.html">
                    Online Fingers Scanner
                  </a>
                
                  <a class="header__dropdown-item"
                     href="/avatars">
                    Online Make Avatars
                  </a>
                
                  <a class="header__dropdown-item"
                     href="/kanban/debounce.html">
                    Online Reminder App
                  </a>
                
                  <a class="header__dropdown-item"
                     href="/converter.html">
                    Online Image Resize
                  </a>
                
              </div>
            </div>

            
          
            
              <a class="header__link" href="/about">
                About
              </a>
            
          
            
              <a class="header__link" href="/contact">
                Contact
              </a>
            
          
            
              <a class="header__link" href="/privacy">
                Privacy Policy
              </a>
            
          
            
              <a class="header__link" href="/disclaimer">
                Disclaimer
              </a>
            
          

        </div>
      </nav>

      <!-- Language Switcher -->
      

      <!-- Mobile Toggle -->
      <div class="header__toggle">
        <span></span><span></span><span></span>
      </div>

    </div>
  </div>

<style>
/* ==================================================
   ðŸ”’ HEADER â€” ALWAYS VISIBLE, FULL WIDTH COLOR
================================================== */
.header{
  position: fixed;
  top: 0;
  left: 0;
  width: 100vw;           /* full width */
  z-index: 10000;
  background: #FFFFFF;    /* uniform header background */
  box-shadow: 0 2px 12px rgba(0,0,0,.35);
  transform: none !important;
  transition: none !important;
}

/* Lock height to avoid reflow */
.header__inner{
  min-height: 72px;
  display: flex;
  align-items: center;
  justify-content: space-between;
}

/* Prevent content hiding under fixed header */
body{
  padding-top: 72px;
}

/* ===============================
   NAV BASE
=============================== */
.header__links-wrapper{
  display:flex;
  align-items:center;
  gap:1.5rem;
}

.header__link,
.header__dropdown-toggle{
  display:inline-flex;
  align-items:center;
  gap:6px;
  cursor:pointer;
  font-size:20px;
  font-weight:500;
}

/* ===============================
   DROPDOWN
=============================== */
.header__dropdown{
  position:relative;
}

.dropdown-arrow{
  font-size:1.4em;
  transition:transform .2s ease;
}

.header__dropdown:hover .dropdown-arrow{
  transform:rotate(180deg);
}

.header__dropdown-menu{
  position:absolute;
  top:100%;
  left:0;
  min-width:220px;
  background:#121212;
  border:1px solid #333;
  border-radius:10px;
  padding:6px 0;
  display:none;
  z-index:9999;
}

.header__dropdown:hover .header__dropdown-menu{
  display:block;
}

.header__dropdown-item{
  display:block;
  padding:10px 16px;
  color:#eaeaea;
  text-decoration:none;
}

.header__dropdown-item:hover{
  background:#1f1f1f;
}

/* ===============================
   MOBILE
=============================== */
@media (max-width:768px){

  body{
    padding-top: 64px;
  }

  .header__inner{
    min-height: 64px;
  }

  .header__links-wrapper{
    flex-direction:column;
    align-items:stretch;
    gap:0;
  }

  .header__dropdown-toggle{
    justify-content:space-between;
    padding:14px 16px;
  }

  .header__dropdown-menu{
    position:static;
    display:none;
    background:#0f0f0f;
    border:none;
  }

  .header__dropdown:hover .header__dropdown-menu{
    display:block;
  }

  .header__dropdown-item{
    padding:14px 20px;
    background:#1a1a1a;
    border-top:1px solid #2a2a2a;
    font-size:15px;
  }
}

/* ===============================
   LANGUAGE SWITCHER
=============================== */
.header__lang-switcher{
  display:inline-flex;
  gap:.5rem;
  align-items:center;
  font-size:1.3rem;
  margin-left:1rem;
}
.lang-flag.active{ text-decoration:underline }
.lang-flag.disabled{ opacity:.5; filter:grayscale(100%) }

</style>
</header>


    
      <div class="hero lazyload"
           data-bg="http://localhost:4000/assets/img/posts/2026-feb-deeprl/2026-feb-deeprl.jpg"
           role="img"
           aria-label="Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL">
    
        <div class="hero__wrap">

          
            <div class="hero__categories">
              
                <a class="label"
                   href="/categories/#Machine Learning"
                   aria-label="Category: Machine Learning">
                   Machine Learning
                </a>
                &nbsp;
              
                <a class="label"
                   href="/categories/#AI"
                   aria-label="Category: AI">
                   AI
                </a>
                &nbsp;
              
                <a class="label"
                   href="/categories/#Python"
                   aria-label="Category: Python">
                   Python
                </a>
                &nbsp;
              
                <a class="label"
                   href="/categories/#Deep RL"
                   aria-label="Category: Deep RL">
                   Deep RL
                </a>
                
              
            </div>
          

          <h1 class="hero__title" itemprop="headline">Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL</h1>

          <p class="hero__meta">
            <time datetime="2026-02-02T00:00:00+08:00" itemprop="datePublished">
              02 Feb 2026
            </time>
            &nbsp;Â·&nbsp;
            
            <span itemprop="timeRequired">
              
                19 mins read
              
            </span>
          </p>
        </div>
      </div>

    <main class="site__content" role="main">
      <div class="container post-layout">

        <!-- SIDEBAR TOC -->
        <aside class="post-toc" id="post-toc">
          <h3 class="no-toc">Contents</h3>
          <nav id="toc-list"></nav>
        </aside>

        <!-- MAIN CONTENT -->
        <article class="post-content"
                 itemprop="articleBody"
                 itemscope itemtype="https://schema.org/Article">

          <!-- INLINE TOC -->
          <div id="inline-toc" class="inline-toc-box">
            <h3 class="no-toc">Contents</h3>
            <nav id="inline-toc-list"></nav>
          </div>

          <h1 id="part-2-markov-decision-processes-explained---mathematical-foundation-of-rl">Part 2: Markov Decision Processes Explained - Mathematical Foundation of RL</h1>

<p>Welcome to the second post in our <strong>Deep Reinforcement Learning Series</strong>! In this comprehensive guide, weâ€™ll explore <strong>Markov Decision Processes (MDPs)</strong> - the mathematical framework that formalizes reinforcement learning problems. Understanding MDPs is crucial for grasping the theoretical foundations of RL.</p>

<h2 id="what-is-a-markov-decision-process">What is a Markov Decision Process?</h2>

<p>A <strong>Markov Decision Process (MDP)</strong> is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs provide a formal way to describe the environment in which reinforcement learning agents operate.</p>

<h3 id="key-properties">Key Properties</h3>

<p><strong>Markov Property:</strong>
The future is independent of the past given the present:
\(P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \dots) = P(S_{t+1} | S_t, A_t)\)</p>

<p>This means the current state contains all information needed to make optimal decisions.</p>

<p><strong>Stationary Transitions:</strong>
Transition dynamics donâ€™t change over time:
\(P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_t, A_t) \quad \forall t\)</p>

<h2 id="formal-definition-of-mdp">Formal Definition of MDP</h2>

<p>An MDP is defined by the tuple:</p>

\[\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)\]

<h3 id="1-state-space-s">1. <strong>State Space (S)</strong></h3>

<p>The set of all possible states the environment can be in:</p>

\[\mathcal{S} = \{s_1, s_2, \dots, s_n\}\]

<p><strong>Types of State Spaces:</strong></p>

<p><strong>Discrete State Space:</strong></p>
<ul>
  <li>Finite number of states</li>
  <li>Examples: Chess board positions, Grid World</li>
  <li>Easier to solve analytically</li>
</ul>

<p><strong>Continuous State Space:</strong></p>
<ul>
  <li>Infinite number of states</li>
  <li>Examples: Robot joint angles, Stock prices</li>
  <li>Requires function approximation</li>
</ul>

<p><strong>Example - Grid World:</strong>
\(\mathcal{S} = \{(x,y) | x \in \{0,1,2,3\}, y \in \{0,1,2,3\}\}\)</p>

<h3 id="2-action-space-a">2. <strong>Action Space (A)</strong></h3>

<p>The set of all possible actions the agent can take:</p>

\[\mathcal{A} = \{a_1, a_2, \dots, a_m\}\]

<p><strong>Types of Action Spaces:</strong></p>

<p><strong>Discrete Action Space:</strong></p>
<ul>
  <li>Finite number of actions</li>
  <li>Examples: Move up/down/left/right, Press button</li>
  <li>Suitable for value-based methods</li>
</ul>

<p><strong>Continuous Action Space:</strong></p>
<ul>
  <li>Infinite action possibilities</li>
  <li>Examples: Robot motor torques, Steering angle</li>
  <li>Requires policy-based methods</li>
</ul>

<p><strong>Example - Grid World:</strong>
\(\mathcal{A} = \{\text{up}, \text{down}, \text{left}, \text{right}\}\)</p>

<h3 id="3-transition-function-p">3. <strong>Transition Function (P)</strong></h3>

<p>The probability of transitioning to sate \(s'\) given current sate \(s\) and action \(a\):</p>

\[\mathcal{P}(s' | s, a) = P(S_{t+1} = s' | S_t = s, A_t = a)\]

<p><strong>Properties:</strong></p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Stochastic:</strong> $$0 \leq P(sâ€™</td>
          <td>s,a) \leq 1$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Normalized:</strong> \(\sum_{s' \in \mathcal{S}} P(s'|s,a) = 1\)
<strong>Example - Deterministic Transitions:</strong></li>
</ul>

\[P(s'|s,a) = \begin{cases}
1 &amp; \text{if } s' = f(s,a) \\
0 &amp; \text{otherwise}
\end{cases}\]

<p>Where \(f(s,a)\) is the deterministic transition function.</p>

<p><strong>Example - Stochastic Transitions:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 80% chance to move in intended direction
# 20% chance to move in random direction
</span><span class="n">P</span><span class="p">(</span><span class="n">s</span><span class="s">'|s,a) = {
    intended_direction: 0.8,
    other_directions: 0.2/3
}
</span></code></pre></div></div>

<h3 id="4-reward-function-r">4. <strong>Reward Function (R)</strong></h3>

<p>The expected immediate reward after taking action \(a\) in sate \(s\) and transitioning to \(s'\):</p>

\[\mathcal{R}(s, a, s') = \mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s']\]

<p><strong>Types of Rewards:</strong></p>

<p><strong>Deterministic Rewards:</strong>
\(R(s,a,s') = r \quad \text{(fixed value)}\)</p>

<p><strong>Stochastic Rewards:</strong>
\(R(s,a,s') = \mathbb{E}[R | s,a,s']\)</p>

<p><strong>Example - Grid World:</strong>
\(R(s,a,s') = \begin{cases}
+10 &amp; \text{if } s' = \text{goal} \\
-1 &amp; \text{if } s' = \text{obstacle} \\
-0.1 &amp; \text{otherwise (time penalty)}
\end{cases}\)</p>

<h3 id="5-discount-factor-Î³">5. <strong>Discount Factor (Î³)</strong></h3>

<p>A factor \(\gamma \in [0,1]\) that determines the importance of future rewards:</p>

<p><strong>Interpretation:</strong></p>
<ul>
  <li>\(\gamma \approx 0\): Agent is myopic (focus on immediate rewards)</li>
  <li>\(\gamma \approx 1\): Agent is farsighted (considers long-term rewards)</li>
  <li>\(\gamma = 1\): Values all future rewards equally</li>
</ul>

<p><strong>Typical Values:</strong></p>
<ul>
  <li>\(\gamma = 0.9\): Moderate discounting</li>
  <li>\(\gamma = 0.95\): Strong long-term consideration</li>
  <li>\(\gamma = 0.99\): Very long-term planning</li>
</ul>

<h2 id="the-mdp-dynamics">The MDP Dynamics</h2>

<h3 id="episode-trajectory">Episode Trajectory</h3>

<p>A sequence of states, actions, and rewards:</p>

\[\tau = (S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T)\]

<h3 id="return-cumulative-reward">Return (Cumulative Reward)</h3>

<p>The sum of discounted rewards from time step \(t\):</p>

\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]

<p><strong>Finite Horizon (Episodic):</strong>
\(G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}\)</p>

<p><strong>Infinite Horizon (Continuing):</strong>
\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)</p>

<h2 id="value-functions">Value Functions</h2>

<h3 id="1-state-value-function-v">1. <strong>State-Value Function (V)</strong></h3>

<p>The expected return starting from sate \(s\) and following policy \(\pi\):</p>

\[V^\pi(s) = \mathbb{E}_\pi \left[ G_t | S_t = s \right]\]

<p><strong>Bellman Expectation Equation:</strong></p>

\[V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]\]

<p><strong>Optimal State-Value Function:</strong></p>

\[V^*(s) = \max_\pi V^\pi(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right]\]

<h3 id="2-action-value-function-q">2. <strong>Action-Value Function (Q)</strong></h3>

<p>The expected return starting from sate \(s\), taking action \(a\), and then following policy \(\pi\):</p>

\[Q^\pi(s,a) = \mathbb{E}_\pi \left[ G_t | S_t = s, A_t = a \right]\]

<p><strong>Bellman Expectation Equation:</strong></p>

\[Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s',a') \right]\]

<p><strong>Optimal Action-Value Function:</strong></p>

\[Q^*(s,a) = \max_\pi Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s',a') \right]\]

<h3 id="3-relationship-between-v-and-q">3. <strong>Relationship Between V and Q</strong></h3>

\[V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s,a)\]

\[Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]\]

<h2 id="policies">Policies</h2>

<h3 id="definition">Definition</h3>

<p>A policy \(\pi\) is a mapping from states to actions:</p>

\[\pi: \mathcal{S} \rightarrow \mathcal{A}\]

<h3 id="types-of-policies">Types of Policies</h3>

<p><strong>Deterministic Policy:</strong>
\(\pi(s) = a \quad \text{(single action)}\)</p>

<p><strong>Stochastic Policy:</strong>
\(\pi(a|s) = P(A_t = a | S_t = s) \quad \text{(probability distribution)}\)</p>

<p><strong>Optimal Policy:</strong></p>

\[\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s,a)\]

<h2 id="bellman-equations">Bellman Equations</h2>

<h3 id="bellman-expectation-equation-for-v">Bellman Expectation Equation for V</h3>

\[V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s \right]\]

<p><strong>Expanded Form:</strong></p>

\[V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right]\]

<h3 id="bellman-optimality-equation-for-v">Bellman Optimality Equation for V</h3>

\[V^*(s) = \max_{a \in \mathcal{A}} \mathbb{E} \left[ R_{t+1} + \gamma V^*(S_{t+1}) | S_t = s, A_t = a \right]\]

<p><strong>Expanded Form:</strong></p>

\[V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right]\]

<h3 id="bellman-expectation-equation-for-q">Bellman Expectation Equation for Q</h3>

\[Q^\pi(s,a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma Q^\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a \right]\]

<p><strong>Expanded Form:</strong></p>

\[Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s',a') \right]\]

<h3 id="bellman-optimality-equation-for-q">Bellman Optimality Equation for Q</h3>

\[Q^*(s,a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a' \in \mathcal{A}} Q^*(S_{t+1}, a') | S_t = s, A_t = a \right]\]

<p><strong>Expanded Form:</strong></p>

\[Q^*(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s',a') \right]\]

<h2 id="python-implementation">Python Implementation</h2>

<h3 id="mdp-class">MDP Class</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">class</span> <span class="nc">MDP</span><span class="p">:</span>
    <span class="s">"""
    Markov Decision Process implementation
    
    Args:
        states: List of all possible states
        actions: List of all possible actions
        transitions: Transition function P(s'|s,a)
        rewards: Reward function R(s,a,s')
        gamma: Discount factor
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> 
                 <span class="n">states</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> 
                 <span class="n">actions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                 <span class="n">transitions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                 <span class="n">rewards</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">states</span> <span class="o">=</span> <span class="n">states</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">transitions</span> <span class="o">=</span> <span class="n">transitions</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">rewards</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_states</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_transition_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                          <span class="n">next_state</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="s">"""
        Get transition probability P(s'|s,a)
        
        Args:
            state: Current state
            action: Action taken
            next_state: Next state
            
        Returns:
            Transition probability
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">transitions</span><span class="p">.</span><span class="n">get</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                 <span class="n">next_state</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="s">"""
        Get expected reward R(s,a,s')
        
        Args:
            state: Current state
            action: Action taken
            next_state: Next state
            
        Returns:
            Expected reward
        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="n">get</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_expected_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="s">"""
        Get expected reward for state-action pair
        
        Args:
            state: Current state
            action: Action taken
            
        Returns:
            Expected reward
        """</span>
        <span class="n">expected_reward</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
            <span class="n">prob</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_transition_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
            <span class="n">expected_reward</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">reward</span>
        <span class="k">return</span> <span class="n">expected_reward</span>
</code></pre></div></div>

<h3 id="value-iteration-algorithm">Value Iteration Algorithm</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> 
                 <span class="n">max_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="s">"""
    Solve MDP using Value Iteration
    
    Args:
        mdp: Markov Decision Process
        theta: Convergence threshold
        max_iterations: Maximum iterations
        
    Returns:
        (optimal_values, optimal_policy)
    """</span>
    <span class="c1"># Initialize value function
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">)</span>
        
        <span class="c1"># Update each state
</span>        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
            <span class="c1"># Bellman optimality equation
</span>            <span class="n">max_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">actions</span><span class="p">:</span>
                <span class="c1"># Expected value for this action
</span>                <span class="n">action_value</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_expected_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                
                <span class="c1"># Add discounted future value
</span>                <span class="k">for</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
                    <span class="n">prob</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_transition_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
                    <span class="n">action_value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">mdp</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
                
                <span class="n">max_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_value</span><span class="p">,</span> <span class="n">action_value</span><span class="p">)</span>
            
            <span class="n">V_new</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_value</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">V_new</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>
        
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
        
        <span class="c1"># Check convergence
</span>        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Converged in </span><span class="si">{</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> iterations"</span><span class="p">)</span>
            <span class="k">break</span>
    
    <span class="c1"># Extract optimal policy
</span>    <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
        <span class="n">max_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
        <span class="n">best_action</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">actions</span><span class="p">:</span>
            <span class="n">action_value</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_expected_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_transition_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
                <span class="n">action_value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">mdp</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
            
            <span class="k">if</span> <span class="n">action_value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                <span class="n">max_value</span> <span class="o">=</span> <span class="n">action_value</span>
                <span class="n">best_action</span> <span class="o">=</span> <span class="n">action</span>
        
        <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_action</span>
    
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">policy</span>
</code></pre></div></div>

<h3 id="policy-iteration-algorithm">Policy Iteration Algorithm</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">max_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">]:</span>
    <span class="s">"""
    Solve MDP using Policy Iteration
    
    Args:
        mdp: Markov Decision Process
        max_iterations: Maximum iterations
        
    Returns:
        (optimal_values, optimal_policy)
    """</span>
    <span class="c1"># Initialize random policy
</span>    <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mdp</span><span class="p">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        <span class="c1"># Policy Evaluation
</span>        <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">True</span>
        
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Fixed iterations for evaluation
</span>            <span class="n">V_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="p">.</span><span class="n">n_states</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
                
                <span class="c1"># Bellman expectation equation
</span>                <span class="n">value</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_expected_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
                    <span class="n">prob</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_transition_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
                    <span class="n">value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">mdp</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
                
                <span class="n">V_new</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            
            <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>
        
        <span class="c1"># Policy Improvement
</span>        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
            <span class="n">old_action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
            <span class="n">max_value</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
            <span class="n">best_action</span> <span class="o">=</span> <span class="mi">0</span>
            
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">actions</span><span class="p">:</span>
                <span class="n">action_value</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_expected_reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="n">mdp</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>
                    <span class="n">prob</span> <span class="o">=</span> <span class="n">mdp</span><span class="p">.</span><span class="n">get_transition_prob</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
                    <span class="n">action_value</span> <span class="o">+=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">mdp</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
                
                <span class="k">if</span> <span class="n">action_value</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">:</span>
                    <span class="n">max_value</span> <span class="o">=</span> <span class="n">action_value</span>
                    <span class="n">best_action</span> <span class="o">=</span> <span class="n">action</span>
            
            <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_action</span>
            
            <span class="k">if</span> <span class="n">old_action</span> <span class="o">!=</span> <span class="n">best_action</span><span class="p">:</span>
                <span class="n">policy_stable</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="k">if</span> <span class="n">policy_stable</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Converged in </span><span class="si">{</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s"> iterations"</span><span class="p">)</span>
            <span class="k">break</span>
    
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">policy</span>
</code></pre></div></div>

<h3 id="example-grid-world-mdp">Example: Grid World MDP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_grid_world_mdp</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">MDP</span><span class="p">:</span>
    <span class="s">"""
    Create a 4x4 Grid World MDP
    
    Returns:
        MDP instance
    """</span>
    <span class="c1"># States: 16 positions (4x4 grid)
</span>    <span class="n">states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
    
    <span class="c1"># Actions: 0=up, 1=down, 2=left, 3=right
</span>    <span class="n">actions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
    
    <span class="c1"># Define transitions and rewards
</span>    <span class="n">transitions</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">state</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span> <span class="n">state</span> <span class="o">%</span> <span class="mi">4</span>
        
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
            <span class="c1"># Calculate next position
</span>            <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># up
</span>                <span class="n">next_x</span><span class="p">,</span> <span class="n">next_y</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># down
</span>                <span class="n">next_x</span><span class="p">,</span> <span class="n">next_y</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># left
</span>                <span class="n">next_x</span><span class="p">,</span> <span class="n">next_y</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># right
</span>                <span class="n">next_x</span><span class="p">,</span> <span class="n">next_y</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span>
            
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">next_x</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">next_y</span>
            
            <span class="c1"># Deterministic transition
</span>            <span class="n">transitions</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1.0</span>
            
            <span class="c1"># Define rewards
</span>            <span class="k">if</span> <span class="n">next_state</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># Goal at (0,3)
</span>                <span class="n">rewards</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">10.0</span>
            <span class="k">elif</span> <span class="n">next_state</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>  <span class="c1"># Obstacle at (1,1)
</span>                <span class="n">rewards</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">rewards</span><span class="p">[(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.1</span>  <span class="c1"># Time penalty
</span>    
    <span class="k">return</span> <span class="n">MDP</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">transitions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Create and solve MDP
</span><span class="n">mdp</span> <span class="o">=</span> <span class="n">create_grid_world_mdp</span><span class="p">()</span>

<span class="c1"># Solve using Value Iteration
</span><span class="n">V</span><span class="p">,</span> <span class="n">policy</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Optimal Values:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Optimal Policy:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># Visualize policy
</span><span class="n">action_symbols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'â†‘'</span><span class="p">,</span> <span class="s">'â†“'</span><span class="p">,</span> <span class="s">'â†'</span><span class="p">,</span> <span class="s">'â†’'</span><span class="p">]</span>
<span class="n">policy_grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">action_symbols</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">row</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">policy</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)])</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Policy Visualization:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">policy_grid</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">row</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="solving-mdps">Solving MDPs</h2>

<h3 id="dynamic-programming-methods">Dynamic Programming Methods</h3>

<p><strong>Value Iteration:</strong></p>
<ul>
  <li>Iteratively improves value function</li>
  <li>Converges to optimal values</li>
  <li>Guarantees optimality</li>
</ul>

<p><strong>Policy Iteration:</strong></p>
<ul>
  <li>Alternates between evaluation and improvement</li>
  <li>Often faster convergence</li>
  <li>Guarantees optimality</li>
</ul>

<h3 id="linear-programming">Linear Programming</h3>

<p>Formulate as linear program:</p>
<ul>
  <li>Variables: State values</li>
  <li>Constraints: Bellman equations</li>
  <li>Objective: Maximize expected return</li>
</ul>

<h3 id="monte-carlo-methods">Monte Carlo Methods</h3>

<p>Estimate values through sampling:</p>
<ul>
  <li>No model required</li>
  <li>Model-free approach</li>
  <li>Converges with enough samples</li>
</ul>

<h3 id="temporal-difference-learning">Temporal Difference Learning</h3>

<p>Combine ideas from DP and MC:</p>
<ul>
  <li>Learn from experience</li>
  <li>Online learning</li>
  <li>Basis for Q-learning</li>
</ul>

<h2 id="example-small-mdp">Example: Small MDP</h2>

<h3 id="problem-setup">Problem Setup</h3>

<p>Consider a simple 3-state MDP:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   S0    â”‚
    â”‚ (Start) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
  S1        S2
 (Good)    (Bad)
</code></pre></div></div>

<p><strong>States:</strong> \(\mathcal{S} = \{S_0, S_1, S_2\}\)</p>

<p><strong>Actions:</strong> \(\mathcal{A} = \{\text{left}, \text{right}\}\)</p>

<p><strong>Transitions:</strong></p>
<ul>
  <li>From \(S_0\): 50% to \(S_1\), 50% to \(S_2\)</li>
  <li>From \(S_1\): Stay in \(S_1\) (terminal)</li>
  <li>From \(S_2\): Stay in \(S_2\) (terminal)</li>
</ul>

<p><strong>Rewards:</strong></p>
<ul>
  <li>
\[R(S_0, \cdot, S_1) = +1\]
  </li>
  <li>
\[R(S_0, \cdot, S_2) = -1\]
  </li>
  <li>
\[R(S_1, \cdot, S_1) = 0\]
  </li>
  <li>
\[R(S_2, \cdot, S_2) = 0\]
  </li>
</ul>

<p><strong>Discount:</strong> \(\gamma = 0.9\)</p>

<h3 id="solving-with-bellman-equations">Solving with Bellman Equations</h3>

<p><strong>Value Function:</strong></p>

<p>\(V(S_0) = 0.5 \times [1 + 0.9 \times V(S_1)] + 0.5 \times [-1 + 0.9 \times V(S_2)]\)
\(V(S_1) = 0\)
\(V(S_2) = 0\)</p>

<p><strong>Substituting:</strong></p>

<p>\(V(S_0) = 0.5 \times [1 + 0.9 \times 0] + 0.5 \times [-1 + 0.9 \times 0]\)
\(V(S_0) = 0.5 \times 1 + 0.5 \times (-1)\)
\(V(S_0) = 0$**Optimal Policy:**$\pi^*(S_0) = \text{left} \quad \text{(both actions give same value)}\)</p>

<p><strong>Optimal Policy:</strong>
\(\pi^*(S_0) = \text{left} \quad \text{(both actions give same value)}\)</p>

<h2 id="partially-observable-mdps-pomdps">Partially Observable MDPs (POMDPs)</h2>

<h3 id="definition-1">Definition</h3>

<p>When the agent cannot fully observe the state:</p>

\[\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{P}, \mathcal{R}, \mathcal{Z}, \gamma)\]

<p>Where:</p>
<ul>
  <li>$\mathcal{O}$ - Observation space</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\mathcal{Z}$ - Observation function $P(o</td>
          <td>s,a,sâ€™)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h3 id="belief-state">Belief State</h3>

<p>Agent maintains probability distribution over states:</p>

\[b_t(s) = P(S_t = s | o_0, a_0, o_1, a_1, \dots, o_t)\]

<h3 id="solving-pomdps">Solving POMDPs</h3>

<ul>
  <li><strong>Belief state MDP</strong>: Convert to MDP over beliefs</li>
  <li><strong>Point-based value iteration</strong>: Approximate solution</li>
  <li><strong>Monte Carlo methods</strong>: Sample-based approaches</li>
</ul>

<h2 id="mdp-properties">MDP Properties</h2>

<h3 id="finite-horizon">Finite Horizon</h3>

<ul>
  <li>Fixed number of time steps</li>
  <li>Episode terminates after \(T\) steps</li>
  <li>Value function depends on time to go</li>
</ul>

<h3 id="infinite-horizon">Infinite Horizon</h3>

<ul>
  <li>No fixed termination</li>
  <li>Discounting ensures convergence</li>
  <li>Stationary optimal policy</li>
</ul>

<h3 id="episodic-vs-continuing">Episodic vs Continuing</h3>

<p><strong>Episodic:</strong></p>
<ul>
  <li>Natural termination points</li>
  <li>Episodes are independent</li>
  <li>Example: Games</li>
</ul>

<p><strong>Continuing:</strong></p>
<ul>
  <li>No natural termination</li>
  <li>Goes on indefinitely</li>
  <li>Example: Robot control</li>
</ul>

<h2 id="whats-next">Whatâ€™s Next?</h2>

<p>In the next post, weâ€™ll implement <strong>Q-Learning from Scratch</strong> - a model-free algorithm that learns optimal policies through experience. Weâ€™ll cover:</p>

<ul>
  <li>Q-learning algorithm</li>
  <li>Exploration strategies</li>
  <li>Implementation details</li>
  <li>Practical examples</li>
  <li>Hyperparameter tuning</li>
</ul>

<h2 id="key-takeaways">Key Takeaways</h2>

<p><strong>MDPs formalize</strong> RL problems mathematically
 <strong>Bellman equations</strong> provide recursive structure
 <strong>Value functions</strong> measure expected returns
 <strong>Dynamic programming</strong> can solve MDPs exactly
 <strong>Model-free methods</strong> learn from experience
 <strong>POMDPs</strong> handle partial observability</p>

<h2 id="practice-exercises">Practice Exercises</h2>

<ol>
  <li><strong>Implement Value Iteration</strong> for a different MDP</li>
  <li><strong>Compare Value vs Policy Iteration</strong> convergence speed</li>
  <li><strong>Create a POMDP</strong> and implement belief updates</li>
  <li><strong>Experiment with different discount factors</strong></li>
  <li><strong>Visualize value functions</strong> as heatmaps</li>
</ol>

<h2 id="questions">Questions?</h2>

<p>Have questions about MDPs or Bellman equations? Drop them in the comments below!</p>

<p><strong>Next Post:</strong> <a href="/Q-Learning-from-Scratch/">Part 3: Q-Learning from Scratch</a></p>

<p><strong>Series Index:</strong> <a href="/Deep-RL-Series-Roadmap/">Deep Reinforcement Learning Series Roadmap</a></p>


          <!-- Syntax Highlighting -->
          <link href="/assets/css/syntax.css" rel="stylesheet">
          <script src="/assets/scripts/copyCode.js" async></script>

          <!-- Ads -->
          <section aria-label="Advertisements" class="ads mt-4">
            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
            <ins class="adsbygoogle"
                 style="display:block"
                 data-ad-client="ca-pub-2976211678184829"
                 data-ad-slot="7658460605"
                 data-ad-format="auto"
                 data-full-width-responsive="true"></ins>
            <script>
              (adsbygoogle = window.adsbygoogle || []).push({});
            </script>
          </section>

        </article>

      </div>

      <!-- Post Navigation -->
      <nav class="post-navigation controls__inner mt-5" aria-label="Post navigation">
        <div class="controls__item prev">
          
            <span>Previous</span>
            <a href="/Introduction-to-Reinforcement-Learning/" rel="prev">
              Part 1: Introduction to Reinforcement...
            </a>
          
        </div>

        <div class="controls__item next">
          
            <span>Next</span>
            <a href="/Q-Learning-from-Scratch/" rel="next">
              Part 3: Q-Learning from Scratch - Com...
            </a>
          
        </div>
      </nav>

      <!-- Disqus -->
      
        <section class="comments mt-5" aria-label="Comments Section">
          
<section class="comments mt-5" aria-label="Comments Section" style="text-align: center; padding: 50px 0; background-color: #fafafa;">
  <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"></div>
  <script>

var disqus_config = function () {
this.page.url = 'http://localhost:4000/Markov-Decision-Processes-Explained/';
this.page.identifier = 'http://localhost:4000/Markov-Decision-Processes-Explained/';
};

(function() {
var d = document, s = d.createElement('script');
s.src = 'https://https-py2ai-github-io.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the comments.</noscript>
</section>


        </section>
      

    </main>

    <footer class="footer">
  <div class="container">
    <nav class="social">
      
      
      
    </nav>
    <span>&copy; 2026 PyShine. All rights reserved.</span>
  </div>
</footer>
<script async src="/assets/js/bundle.js"></script>

<script async>
  if ('serviceWorker' in navigator) {
    navigator.serviceWorker.register('/sw.js').then(function( registration ) {
      console.log('ServiceWorker registration successful with scope: ', registration.scope);
    })
    .catch(function(error) {
      console.log('ServiceWorker registration failed: ', error);
    });
  }
</script>



    <!-- MathJax -->
    


    <!-- Ads -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

    <!-- SIDEBAR TOC SCRIPT -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      const toc = document.getElementById("toc-list");
      const headings = document.querySelectorAll(
        ".post-content h2:not(.no-toc), .post-content h3:not(.no-toc)"
      );
      if (!toc || headings.length === 0) return;

      const ul = document.createElement("ul");
      headings.forEach(h => {
        const id = h.id || h.textContent.trim().toLowerCase().replace(/\s+/g, "-");
        h.id = id;

        const li = document.createElement("li");
        if (h.tagName === "H3") li.style.marginLeft = "1rem";

        li.innerHTML = `<a href="#${id}">${h.textContent}</a>`;
        ul.appendChild(li);
      });
      toc.appendChild(ul);
    });
    </script>

    <!-- INLINE TOC SCRIPT -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      const tocTarget = document.getElementById("inline-toc-list");
      const headings = document.querySelectorAll(
        ".post-content h2:not(.no-toc), .post-content h3:not(.no-toc)"
      );
      if (!tocTarget || headings.length === 0) return;

      const ul = document.createElement("ul");
      headings.forEach(h => {
        const id = h.id || h.textContent.trim().toLowerCase().replace(/\s+/g, "-");
        h.id = id;

        const li = document.createElement("li");
        if (h.tagName === "H3") li.style.marginLeft = "1rem";

        li.innerHTML = `<a href="#${id}">${h.textContent}</a>`;
        ul.appendChild(li);
      });
      tocTarget.appendChild(ul);
    });
    </script>

    <!-- INLINE TOC STYLES -->
    <style>
      .inline-toc-box {
        background: #D4EDFD;
        padding: 15px 20px;
        border-radius: 10px;
        margin-bottom: 25px;
        border: 1px solid #e0e0e0;
      }
      .inline-toc-box ul {
        margin: 0;
        padding-left: 18px;
      }
      .inline-toc-box li {
        margin-bottom: 6px;
      }
      /* Center post navigation */
      .post-navigation.controls__inner {
        justify-content: center !important;
        gap: 4rem;
      }
    </style>

  </body>
</html>


